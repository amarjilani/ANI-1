{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "da9b89d0",
      "metadata": {
        "id": "da9b89d0"
      },
      "source": [
        "# 277B Final - Molecular Energy Prediction\n",
        "Amar Jilani"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tvCzWQ6OfX1k",
      "metadata": {
        "id": "tvCzWQ6OfX1k"
      },
      "source": [
        "Notes:\n",
        "- Trained on molecules up to 4 heavy atoms \n",
        "- Tested on 5 heavy atoms dataset\n",
        "\n",
        "\n",
        ">Note: Apologies for no output cells, kernel crashed but running on the s01 file is pretty fast if you want to try"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f84058c",
      "metadata": {
        "id": "7f84058c"
      },
      "source": [
        "#### Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "010f0538",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "010f0538",
        "outputId": "3badf21e-6afd-4fc0-f677-e3fd4c527b69"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# FOR GOOGLE COLAB ONLY \n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eP4EXxSkg6KA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eP4EXxSkg6KA",
        "outputId": "ef9503b5-5a6a-4a5e-dcab-784a90a3a44b"
      },
      "outputs": [],
      "source": [
        "# FOR GOOGLE COLAB\n",
        "# !pip install torchani"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WeQUYI_igO50",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeQUYI_igO50",
        "outputId": "e68b5e01-189b-4a87-e3f1-fc96f5220d78"
      },
      "outputs": [],
      "source": [
        "# SAVIO\n",
        "# sys.path.append(\"/global/scratch/users/amarjilani/ANI-dataset/ANI-1_release\")\n",
        "\n",
        "# COLAB\n",
        "# sys.path.append(\"/content/drive/MyDrive/\")\n",
        "\n",
        "import pyanitools as pya\n",
        "import torchani"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d758efa8",
      "metadata": {
        "id": "d758efa8"
      },
      "outputs": [],
      "source": [
        "# parameters are from rHCNO-5.2R_16-3.5A_a4-8.params in the torchani repository\n",
        "# https://github.com/aiqm/torchani/blob/master/torchani/resources/ani-1x_8x/rHCNO-5.2R_16-3.5A_a4-8.params\n",
        "Rcr = 5.2\n",
        "Rca = 3.5\n",
        "EtaR = torch.tensor([16], dtype=torch.float)\n",
        "ShfR = torch.tensor([0.900000,1.168750,1.437500,1.706250,1.975000,2.243750,2.51250,2.781250,3.050000,\\\n",
        "                            3.318750,3.587500,3.856250,4.125000,4.39375,4.662500,4.931250], dtype=torch.float)\n",
        "EtaA= torch.tensor([8], dtype=torch.float)\n",
        "Zeta = torch.tensor([32], dtype=torch.float)\n",
        "ShfA = torch.tensor([0.900000,1.550000,2.200000,2.850000], dtype=torch.float)\n",
        "ShfZ = torch.tensor([0.19634954,0.58904862,0.9817477,1.3744468,1.7671459,2.1598449,2.552544,2.945243],\n",
        "                    dtype=torch.float)\n",
        "num_species = 4\n",
        "aev_computer = torchani.AEVComputer(Rcr, Rca, EtaR,\n",
        "                                    ShfR, EtaA, Zeta, ShfA, ShfZ, num_species)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0250865",
      "metadata": {
        "id": "f0250865"
      },
      "outputs": [],
      "source": [
        "# SAVIO SMALL DATASET (use for testing/debugging)\n",
        "# data_directory = '/global/scratch/users/amarjilani/ANI-dataset/ANI-1_release/ani_gdb_s01.h5'\n",
        "\n",
        "# SAVIO PRODUCTION TRAINING SET (s01 to s04)\n",
        "# data_directory = '/global/scratch/users/amarjilani/ANI-dataset/ANI-1_release/training'\n",
        "\n",
        "# COLAB SMALL DATASET (use for testing/debugging)\n",
        "# data_directory = '/content/drive/MyDrive/data/ani_gdb_s01.h5'\n",
        "\n",
        "# COLAB PRODUCTION TRAINING SET (s01 to s04)\n",
        "# data_directory = '/content/drive/MyDrive/data/'\n",
        "\n",
        "# LOCAL TESTING \n",
        "data_directory = '../datasets/ANI-1_release/ani_gdb_s01.h5'\n",
        "\n",
        "# Using TorchANI's built-in data loading functions\n",
        "energy_shifter = torchani.utils.EnergyShifter(None)\n",
        "data = torchani.data.load(data_directory)\n",
        "training, validation = data.subtract_self_energies(energy_shifter).species_to_indices().shuffle().split(0.8, 0.2)\n",
        "training = training.collate(128).cache()\n",
        "validation = validation.collate(128).cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f04c2dd",
      "metadata": {
        "id": "7f04c2dd",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def convert_aev(mol):\n",
        "    \"\"\"\n",
        "    Converts 3D coordinates into AEV representations\n",
        "    Works on single molecules or batches\n",
        "    \"\"\"\n",
        "    elems = mol['species']\n",
        "    aev = aev_computer.forward((torch.tensor(elems, dtype=torch.long),\n",
        "                                     torch.tensor(mol['coordinates'], dtype=torch.float)))\n",
        "    return aev"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cefcb35a",
      "metadata": {
        "id": "cefcb35a"
      },
      "source": [
        "# Primary Model - Predicting Energy based on 3D Coordinates (AEVs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5cd4954",
      "metadata": {
        "id": "a5cd4954"
      },
      "outputs": [],
      "source": [
        "class ANI_sub(nn.Module):\n",
        "    \"\"\"Sub-network for ONE type of atom\"\"\"\n",
        "    def __init__(self, architecture):\n",
        "        super(ANI_sub, self).__init__()\n",
        "        layers = []\n",
        "\n",
        "        # create fully connected layers\n",
        "        for i in range(len(architecture) - 1):\n",
        "            layers.append(nn.Linear(architecture[i], architecture[i + 1]))\n",
        "            layers.append(nn.LeakyReLU()) # tested different activation functions, leakyrelu was the best\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, aev):\n",
        "        # take AEV as input and run through fully connected layer to calculate energy\n",
        "        atomic_energy = self.network(aev)\n",
        "        return atomic_energy\n",
        "\n",
        "class ANI(nn.Module):\n",
        "    \"\"\"Model for calculating the energy of a specific conformation of an organic molecule consisting of H, C, N or O.\"\"\"\n",
        "    def __init__(self, architectures):\n",
        "        super(ANI, self).__init__()\n",
        "        # create subnets for each atom passed in the architectures dictionary\n",
        "        self.sub_nets = nn.ModuleDict({\n",
        "            atom: ANI_sub(architecture) for atom, architecture in architectures.items()\n",
        "        })\n",
        "\n",
        "    def forward(self, aevs, atom_types):\n",
        "        batch_energies = []  # store the total energies for each conformation in batch\n",
        "\n",
        "        # go through each conf in the batch\n",
        "        for conf_atom_types, conf_aevs in zip(atom_types, aevs):\n",
        "            atomic_energies = []\n",
        "\n",
        "            # each conformation conssists of an aev for each atom\n",
        "            for atom_type, aev in zip(conf_atom_types, conf_aevs):\n",
        "                if atom_type != -1:  # exclude padding atoms\n",
        "                    atomic_energy = self.sub_nets[str(atom_type.item())](aev)\n",
        "                    atomic_energies.append(atomic_energy)\n",
        "\n",
        "            # sum the energies of all atoms in the molecule comformer\n",
        "            total_molecule_energy = torch.sum(torch.stack(atomic_energies))\n",
        "            batch_energies.append(total_molecule_energy)\n",
        "\n",
        "        # convert list of energies to a tensor\n",
        "        total_energies = torch.stack(batch_energies)\n",
        "        return total_energies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c37ec4ef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c37ec4ef",
        "outputId": "05d27ae2-713c-468e-8706-1c03d0758509",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# architecture for each subnet, haven't played around with this yet\n",
        "architectures = {\n",
        "    \"0\": [384, 128, 64, 1], # hydrogen\n",
        "    \"1\": [384, 128, 64, 1], # carbon\n",
        "    \"2\":[384, 128, 64, 1], # nitrogen\n",
        "    \"3\":[384, 128, 64, 1] # oxygen\n",
        "}\n",
        "\n",
        "model = ANI(architectures)\n",
        "model = model.float()\n",
        "model.sub_nets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8da4ef3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8da4ef3",
        "outputId": "41e28a92-7e43-460e-90f2-f5630de7a3f0"
      },
      "outputs": [],
      "source": [
        "# Run on GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b59e7bca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b59e7bca",
        "outputId": "8906ed85-afdf-4c93-a79f-975e4b5e797d"
      },
      "outputs": [],
      "source": [
        "# Training script\n",
        "loss_func = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "num_epochs = 20\n",
        "\n",
        "lowest_val = float('inf')\n",
        "weights = model.state_dict()\n",
        "losses = []\n",
        "val_losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    # training\n",
        "    for mol in training:\n",
        "        species, aevs = convert_aev(mol)\n",
        "        species = species.to(device)\n",
        "        aevs = aevs.to(device)\n",
        "        energies = mol['energies'].float().to(device)\n",
        "        predicted_energies = model(aevs, species)\n",
        "        loss = loss_func(predicted_energies, energies)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * species.size(0)\n",
        "    train_loss = train_loss / len(training)\n",
        "    losses.append(train_loss)\n",
        "\n",
        "    # validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for mol in validation:\n",
        "            species, aevs = convert_aev(mol)\n",
        "            species = species.to(device)\n",
        "            aevs = aevs.to(device)\n",
        "            energies = mol['energies'].to(device)\n",
        "            predicted_energies = model(aevs, species)\n",
        "            loss = loss_func(predicted_energies, energies)\n",
        "            val_loss += loss.item() * species.size(0)\n",
        "\n",
        "    val_loss = val_loss / len(validation)\n",
        "    val_losses.append(val_loss)\n",
        "    if val_loss < lowest_val:\n",
        "        lowest_val = val_loss\n",
        "        weights = model.state_dict()\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': train_loss,\n",
        "            'val_loss': val_loss\n",
        "        }, f'ani.pth')\n",
        "\n",
        "    # print result\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.7f}, Val Loss: {val_loss:.7f}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "285e29ff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "285e29ff",
        "outputId": "316b8ea3-d977-40d5-e8e4-96479b167c1a"
      },
      "outputs": [],
      "source": [
        "# plot curves\n",
        "plt.plot(val_losses, label=\"Validation Loss\")\n",
        "plt.plot(losses, label=\"Training Loss\") # getting rid of the first bc squishes the plot\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xticks(range(0, 20, 1))\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2wTqcLQVqe08",
      "metadata": {
        "id": "2wTqcLQVqe08"
      },
      "outputs": [],
      "source": [
        "model = model.to(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "361b7a6a",
      "metadata": {
        "id": "361b7a6a"
      },
      "source": [
        "# Alternative Method: AutoEncoder for Compression of AEVs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1433ad2e",
      "metadata": {
        "id": "1433ad2e"
      },
      "source": [
        "#### AutoEncoder class\n",
        "I chose to use a similar architecture to the previous neural network, where there are sub-nets that focus on a specific atom. This way, each AEV in a molecule is being compressed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40218248",
      "metadata": {
        "id": "40218248"
      },
      "outputs": [],
      "source": [
        "class AtomEncoder(nn.Module):\n",
        "    \"\"\"Sub-AutoEncoder for a specific atom type\"\"\"\n",
        "    def __init__(self, architecture):\n",
        "        super(AtomEncoder, self).__init__()\n",
        "        encoder_layers = []\n",
        "        decoder_layers = []\n",
        "\n",
        "        # encoder\n",
        "        for i in range(len(architecture) - 1):\n",
        "            encoder_layers.append(nn.Linear(architecture[i], architecture[i + 1]))\n",
        "            encoder_layers.append(nn.LeakyReLU())\n",
        "        self.encoder = nn.Sequential(*encoder_layers[:-1]) # don't use activation on the latent space\n",
        "\n",
        "        # decoder\n",
        "        for i in range(len(architecture) - 1, 0, -1):\n",
        "            decoder_layers.append(nn.Linear(architecture[i], architecture[i - 1]))\n",
        "            decoder_layers.append(nn.LeakyReLU())\n",
        "        self.decoder = nn.Sequential(*decoder_layers[:-1])\n",
        "\n",
        "    def forward(self, aev):\n",
        "        encoded = self.encoder(aev)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return encoded, decoded\n",
        "\n",
        "class MolEncoder(nn.Module):\n",
        "    \"\"\"AutoEncoder network for a molecule, with sub-encoders for each specific atom\"\"\"\n",
        "    def __init__(self, architectures):\n",
        "        super(MolEncoder, self).__init__()\n",
        "        self.sub_encoders = nn.ModuleDict({\n",
        "            atom: AtomEncoder(architecture) for atom, architecture in architectures.items()\n",
        "        })\n",
        "\n",
        "    def forward(self, aevs, atom_types):\n",
        "        batch_encoded = []\n",
        "        batch_decoded = []\n",
        "        for conf_atom_types, conf_aevs in zip(atom_types, aevs):\n",
        "            conf_encoded = []\n",
        "            conf_decoded = []\n",
        "\n",
        "            for atom_type, aev in zip(conf_atom_types, conf_aevs):\n",
        "                if atom_type != -1:  # ignore padding atoms\n",
        "                    encoded, decoded = self.sub_encoders[str(atom_type.item())](aev)\n",
        "                else:\n",
        "                    encoded = torch.zeros(16, device=device)  # replace with appropriate encoded size\n",
        "                    decoded = torch.zeros_like(aev, device=device)  # decoded padding should match AEV padding\n",
        "\n",
        "                conf_encoded.append(encoded)\n",
        "                conf_decoded.append(decoded)\n",
        "\n",
        "            # concat encoded and decoded outputs for each molecule\n",
        "            combined_encoded = torch.stack(conf_encoded)\n",
        "            combined_decoded = torch.cat(conf_decoded)\n",
        "            combined_decoded = combined_decoded.view_as(conf_aevs)\n",
        "            batch_encoded.append(combined_encoded)\n",
        "            batch_decoded.append(combined_decoded)\n",
        "\n",
        "        # convert to tensors\n",
        "        encoded_batch = torch.stack(batch_encoded)\n",
        "        decoded_batch = torch.stack(batch_decoded)\n",
        "        return encoded_batch, decoded_batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f31b984c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f31b984c",
        "outputId": "91af35f9-d334-4232-dc86-202c146e469c"
      },
      "outputs": [],
      "source": [
        "ae_architectures = {\n",
        "    \"0\": [384, 128, 64, 32, 16], # hydrogen\n",
        "    \"1\": [384, 128, 64, 32, 16], # carbon\n",
        "    \"2\":[384, 128, 64, 32, 16], # nitrogen\n",
        "    \"3\":[384, 128, 64, 32, 16] # oxygen\n",
        "}\n",
        "autoencoder = MolEncoder(ae_architectures)\n",
        "autoencoder = autoencoder.to(device)\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CSIsBNvwvMjR",
      "metadata": {
        "id": "CSIsBNvwvMjR"
      },
      "outputs": [],
      "source": [
        "ae_load = torch.load(f'ae.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QzondtWpwcad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzondtWpwcad",
        "outputId": "81c70e86-29c0-4330-dfb1-80cd03fcd3b1"
      },
      "outputs": [],
      "source": [
        "ae_load[\"loss\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a872835",
      "metadata": {
        "id": "7a872835"
      },
      "source": [
        "#### Training AutoEncoder network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6690bcd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6690bcd",
        "outputId": "733cf42c-cde3-4df6-a11c-cc96d1a1254c"
      },
      "outputs": [],
      "source": [
        "loss_func = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-3)\n",
        "\n",
        "if ae_load:\n",
        "    autoencoder.load_state_dict(ae_load['model_state_dict'])\n",
        "    optimizer.load_state_dict(ae_load['optimizer_state_dict'])\n",
        "    num_epochs = 20 - ae_load['epoch']\n",
        "    lowest_val = ae_load['val_loss']\n",
        "    losses = [ae_load['loss']]\n",
        "    val_losses = [ae_load['val_loss']]\n",
        "\n",
        "else:\n",
        "    num_epochs = 20\n",
        "\n",
        "    weights = autoencoder.state_dict()\n",
        "    losses = []\n",
        "    val_losses = []\n",
        "    lowest_val = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    autoencoder.train()  \n",
        "    train_loss = 0.0\n",
        "\n",
        "    # training\n",
        "    for mol in training:\n",
        "        species, aevs = convert_aev(mol)\n",
        "        species = species.cuda()\n",
        "        aevs = aevs.cuda()\n",
        "        encoded, decoded = autoencoder(aevs, species)\n",
        "        loss = loss_func(decoded, aevs)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * species.size(0)\n",
        "    train_loss = train_loss / len(training)\n",
        "    losses.append(train_loss)\n",
        "\n",
        "    # val\n",
        "    autoencoder.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for mol in validation:\n",
        "            species, aevs = convert_aev(mol)\n",
        "            species = species.to(device)\n",
        "            aevs = aevs.to(device)\n",
        "            encoded, decoded = autoencoder(aevs, species)\n",
        "            loss = loss_func(decoded, aevs)\n",
        "            val_loss += loss.item() * species.size(0)\n",
        "\n",
        "    val_loss = val_loss / len(validation)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    # save results \n",
        "    if val_loss < lowest_val:\n",
        "        lowest_val = val_loss\n",
        "        weights = autoencoder.state_dict()\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': autoencoder.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': train_loss,\n",
        "            'val_loss': val_loss\n",
        "        }, f'ae.pth')\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.7f}, Val Loss: {val_loss:.7f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0fd7e5a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "e0fd7e5a",
        "outputId": "f5ab53a5-8395-4a67-8db6-4df8b8ff3c89"
      },
      "outputs": [],
      "source": [
        "# autoencoder training/validation curves\n",
        "plt.plot(val_losses, label=\"Validation Loss\")\n",
        "plt.plot(losses, label=\"Training Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cf060f2",
      "metadata": {
        "id": "1cf060f2"
      },
      "source": [
        "#### Evaluating the Encoded AEV Representation on a Second Energy Prediction Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6afa2c76",
      "metadata": {
        "id": "6afa2c76"
      },
      "outputs": [],
      "source": [
        "architectures2 = {\n",
        "    \"0\": [16, 64, 32, 1],\n",
        "    \"1\": [16, 64, 32, 1],\n",
        "    \"2\":[16, 64, 32, 1],\n",
        "    \"3\":[16, 64, 32, 1]\n",
        "}\n",
        "model_2 = ANI(architectures2) # using the same class but with different architectures\n",
        "model_2 = model_2.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc961e63",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dc961e63",
        "outputId": "a28b4452-8df1-4893-b886-01fb280573e4"
      },
      "outputs": [],
      "source": [
        "loss_func = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model_2.parameters(), lr=1e-3)\n",
        "num_epochs = 20\n",
        "\n",
        "lowest_val = float('inf')\n",
        "weights = model_2.state_dict()\n",
        "losses = []\n",
        "val_losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    model_2.train()  # Set the model to training mode\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for mol in training:\n",
        "        species, aevs = convert_aev(mol)\n",
        "        species = species.to(device)\n",
        "        aevs = aevs.to(device)\n",
        "        energies = mol['energies'].float().to(device)\n",
        "        encoded_aevs, _ = autoencoder(aevs, species)\n",
        "        encoded_aevs = encoded_aevs.detach() # don't want to adjust gradients in the autoencoder network\n",
        "        predicted_energies = model_2(encoded_aevs, species)\n",
        "        loss = loss_func(predicted_energies, energies)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * species.size(0)\n",
        "    train_loss = train_loss / len(training)\n",
        "    losses.append(train_loss)\n",
        "\n",
        "    # val\n",
        "    model_2.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for mol in validation:\n",
        "            species, aevs = convert_aev(mol)\n",
        "            species = species.to(device)\n",
        "            aevs = aevs.to(device)\n",
        "            energies = mol['energies'].float().to(device)\n",
        "            encoded_aevs, _ = autoencoder(aevs, species)\n",
        "            predicted_energies = model_2(encoded_aevs, species)\n",
        "            loss = loss_func(predicted_energies, energies)\n",
        "            val_loss += loss.item() * species.size(0)\n",
        "\n",
        "    val_loss = val_loss / len(validation)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    # save model\n",
        "    if val_loss < lowest_val:\n",
        "        lowest_val = val_loss\n",
        "        weights = model_2.state_dict()\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model_2.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': train_loss,\n",
        "            'val_loss': val_loss\n",
        "        }, f'model_2.pth')\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.7f}, Val Loss: {val_loss:.7f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7067646",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "e7067646",
        "outputId": "13431abf-a9b6-4728-f1ed-2b905e6112cc"
      },
      "outputs": [],
      "source": [
        "# plot model 2 training and validation curves\n",
        "plt.plot(val_losses, label=\"Validation Loss\")\n",
        "plt.plot(losses, label=\"Training Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4d43086",
      "metadata": {
        "id": "d4d43086"
      },
      "source": [
        "## Testing on 5 Heavy Atoms dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ac2dc4d",
      "metadata": {},
      "source": [
        "Now that the models have been trained on data up to 4 heavy atoms, we will look at how the models can generalize to larger molecules"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba42a3b5",
      "metadata": {},
      "source": [
        "#### Testing the Original Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0a7e6ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "# load in trained weights\n",
        "ani_load = torch.load('ani.pth')\n",
        "model.load_state_dict(ani_load['model_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8ff58c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# load in 5 heavy atom set \n",
        "data_directory = '../datasets/ANI-1_release/ani_gdb_s05.h5'\n",
        "energy_shifter = torchani.utils.EnergyShifter(None)\n",
        "data = torchani.data.load(data_directory)\n",
        "test = data.subtract_self_energies(energy_shifter).species_to_indices().shuffle()\n",
        "test = test.collate(1024).cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bec50a40",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "model.eval()\n",
        "sum_squared_error = 0.0\n",
        "total_samples = 0\n",
        "actual_energies = []\n",
        "predicted_energies_list = []\n",
        "batch_counter = 0\n",
        "max_batches = 20    # only going through 1024*20 of the conformations in dataset 5 \n",
        "timings = []\n",
        "with torch.no_grad():\n",
        "    for mol in test:\n",
        "        if batch_counter >= max_batches:\n",
        "            break\n",
        "        \n",
        "        # make prediction \n",
        "        species, aevs = convert_aev(mol)\n",
        "        energies = mol['energies']\n",
        "        start = time.time()\n",
        "        predicted_energies = model(aevs, species)\n",
        "        end = time.time()\n",
        "\n",
        "        # store values for plotting \n",
        "        actual_energies.extend(energies.numpy())\n",
        "        predicted_energies_list.extend(predicted_energies.numpy())\n",
        "\n",
        "        # calculate squared errors \n",
        "        squared_errors = (predicted_energies - energies) ** 2\n",
        "        sum_squared_error += squared_errors.sum().item()\n",
        "        total_samples += energies.size(0)\n",
        "\n",
        "        batch_counter += 1\n",
        "        timings.append(end - start)\n",
        "\n",
        "# calculate RMSE\n",
        "rmse = np.sqrt(sum_squared_error / total_samples)\n",
        "print(\"RMSE: \", rmse)\n",
        "\n",
        "# calculate average time the model took to make prediction \n",
        "avg_time = np.mean(timings)\n",
        "print(\"Average time taken for compressed model prediction: {:.2f} seconds\".format(avg_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33097802",
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot predicted vs observed \n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(actual_energies, predicted_energies_list, alpha=0.5)\n",
        "plt.xlabel('Actual Energies (Hartrees)')\n",
        "plt.ylabel('Predicted Energies (Hartrees)')\n",
        "plt.title('Original Mode: True vs Predicted Energies')\n",
        "plt.plot([min(actual_energies), max(actual_energies)], [min(actual_energies), max(actual_energies)], 'r')  \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "437cd037",
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "# calculate correlation coefficient \n",
        "correlation, _ = pearsonr(actual_energies, predicted_energies_list)\n",
        "print(\"Model 1 Correlation Coefficient: \", correlation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8b4d4c4",
      "metadata": {},
      "source": [
        "#### Testing the Secondary Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fb538d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# load in saved training data \n",
        "model2 = ANI(architectures2) \n",
        "latent = torch.load('model_2.pth')\n",
        "model2.load_state_dict(latent['model_state_dict'])\n",
        "\n",
        "autoencoder2 = MolEncoder(ae_architectures)\n",
        "ae2 = torch.load('ae.pth')\n",
        "autoencoder2.load_state_dict(ae2['model_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08d77889",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time \n",
        "model2.eval()\n",
        "sum_squared_error = 0.0\n",
        "total_samples = 0\n",
        "actual_energies = []            # stores values for plotting\n",
        "predicted_energies_list = []    # stores values for plotting \n",
        "batch_counter = 0\n",
        "max_batches = 20    # only going through 1024*20 of the conformations in dataset 5 \n",
        "timings = []    # stores the timing taken for predictions \n",
        "\n",
        "with torch.no_grad():\n",
        "    for mol in test:\n",
        "        if batch_counter >= max_batches:\n",
        "            break\n",
        "        \n",
        "        # make prediction \n",
        "        species, aevs = convert_aev(mol)\n",
        "        encoded_aevs, _ = autoencoder2(aevs, species)\n",
        "        energies = mol['energies']\n",
        "        start_time = time.time()\n",
        "        predicted_energies = model2(encoded_aevs, species)\n",
        "        end_time = time.time()\n",
        "        timings.append(end_time - start_time)\n",
        "\n",
        "        # store values for plotting \n",
        "        actual_energies.extend(energies.numpy())\n",
        "        predicted_energies_list.extend(predicted_energies.numpy())\n",
        "\n",
        "        # calculate squared errors \n",
        "        squared_errors = (predicted_energies - energies) ** 2\n",
        "        sum_squared_error += squared_errors.sum().item()\n",
        "        total_samples += energies.size(0)\n",
        "\n",
        "        batch_counter += 1\n",
        "\n",
        "# calculate RMSE\n",
        "rmse = np.sqrt(sum_squared_error / total_samples)\n",
        "print(\"RMSE: \", rmse)\n",
        "\n",
        "avg_time = np.mean(timings)\n",
        "print(\"Average time taken for compressed model prediction: {:.2f} seconds\".format(avg_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caf61002",
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot model2 prediction vs observed \n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(actual_energies, predicted_energies_list, alpha=0.5)\n",
        "plt.xlabel('Actual Energies (kcal/mol)')\n",
        "plt.ylabel('Predicted Energies (kcal/mol)')\n",
        "plt.title('Encoded Input Model: True vs Predicted Energies')\n",
        "plt.plot([min(actual_energies), max(actual_energies)], [min(actual_energies), max(actual_energies)], 'r')  \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dab7503",
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculate correlation between observed and predicted for model 2\n",
        "correlation, _ = pearsonr(actual_energies, predicted_energies_list)\n",
        "print(\"Model 2 Correlation Coefficient: \", correlation)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
